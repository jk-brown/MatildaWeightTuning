---
title: "Computing Ensemble Likelihood Using Alternative Criteria Weights"
author: "Joe Brown"
date: "2024-09-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal 

The goal of this script is to calculate likelihood of Matilda ensemble using different criteria weights. 

Specifically, we are going to weight the model results many times with the same criteria but applying different weight to the influence of each criteria. We will complete this effort by looping the ensemble likelihood calculation procedure through a full factorial set of criteria weights that sum to 1.

We will perform this procedure with parallel computing by looping through criteria weight values for each iteration. This will substitute a new set of criteria weights with each run. 

## Load data

We are loading the model results and the criteria weights.

```{r load data from data directory}
# model_result <- readRDS("data/model_result.RDS")
# criteria_weights_list <- readRDS("data/criteria_wts.RDS")

```

## Weight Ensembles 

```{r weight model ensembles with different criteria weights}

# initiate a new cluster
cl <- makeCluster(detectCores() - 1)

# export functions and objects to weight ensembles
clusterExport(cl, c("score_runs", "score_bayesian", "criterion_co2_obs", "criterion_ocean_uptake", "criterion_temp", "model_result", "criteria_weights_list", "co2_unc", "gmst_unc", "ocean_uptake_unc", "multi_criteria_weighting"))

# start time
start_time <- Sys.time()

# run likelihood weighting with parLapply
weight_results <- parLapply(cl, criteria_weights_list, function(weight_combo_df) {
  
  # numeric vector of weight combinations
  weight_combo <- as.numeric(weight_combo_df)
  
  # loop across each criteria weight combinations to compute ensemble weights for each combination
  wts_res <- lapply(model_result, function(df) {
    
    # scores from co2
    wts_co2 = score_runs(df, 
                         criterion = criterion_co2_obs(), 
                         score_function = score_bayesian, 
                         sigma = co2_unc)
    wts_co2 = na.omit(wts_co2)
    
    # scores from temp
    wts_temp = score_runs(df, 
                          criterion = criterion_temp, 
                          score_function = score_bayesian,
                          sigma = gmst_unc)
    wts_temp = na.omit(wts_temp)
    
    # scores from ocean_uptake 
    wts_oc_uptake = score_runs(df, 
                               criterion = criterion_ocean_uptake,
                               score_function = score_bayesian,
                               sigma = ocean_uptake_unc)
    wts_temp = na.omit(wts_temp)
    
    # score list 
    score_list = list(wts_co2, wts_temp, wts_oc_uptake)
    
    # multi-criteria scores
    multi_criteria_weighting(score_list, weight_combo)
    
    })
  
  return(wts_res)
  
})

# stop cluster
stopCluster(cl)

# print elapsed time
end_time <- Sys.time()
time_elapsed <- end_time - start_time
print(time_elapsed)

```
Filter out ensembles with weights near-zero == 10e-6

```{r}
# filter out ensemble members that do not meet the minimum weight constraint (> 1e-6)
constrained_weights <- lapply(weight_results, function(criteria_level) {
  
  # iterate over the SSP level
  lapply(criteria_level, function(scenario_level) {
    
    filtered_result <- subset(scenario_level, mc_weight > 1e-6)
    
    return(filtered_result)
    
  })
  
})

# Re-weight to sum to one
final_ensemble_weights <- lapply(weight_results, function(criteria_level) {
  
  # loop over the scenario level
  lapply(criteria_level, function(scenario_level) {
    
    # calculate the total weight for unique run_numbers
    total_weight <- sum(scenario_level$mc_weight[!duplicated(scenario_level$run_number)])
    
    # normalize the weight values
    scenario_level$norm_weight <- scenario_level$mc_weight / total_weight
    
    return(scenario_level)
    
  })
  
  
})
```


Save the weights which will be merged with metrics in a future step:

```{r}
saveRDS(final_ensemble_weights, "data/weight_results.RDS")
```

